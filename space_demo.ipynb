{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "### How to install\n",
    "Spacy can be installed using either `pip` or `conda`:\n",
    "* `pip install -U spacy`\n",
    "* `conda install -c conda-forge spacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.11'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading a Vocabulary Model\n",
    "\n",
    "After importing the library, load one of the many available models. Models may be installed via spaCy's download command\n",
    "\n",
    "```python\n",
    "python -m spacy download <MODEL NAME> \n",
    "\n",
    "```\n",
    "List of available models and available features can be found in the [Available models section of the documentation](https://spacy.io/usage/models#available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the en_core_web_sm model: \n",
    "# https://spacy.io/models/en#en_core_web_md\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "type(nlp).__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the components we need to process text. The next step is to pass in the text data into `nlp` and invoke its various methods appropriate for the analysis we want to undertake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Features\n",
    "\n",
    "## Basic things that spaCy can do\n",
    "* Tokenization (word and sentence)\n",
    "* Lemmatization\n",
    "* Part-of-speech tagger\n",
    "* Depdenency parsing\n",
    "* Named entity recognition\n",
    "\n",
    "For full list, see [this page](https://spacy.io/usage/spacy-101#features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Good morning. Welcome, thanks for joining us for Facebook’s 2018 Annual Meeting of Stockholders. I’m Dave Kling, Deputy General Counsel and Corporate Secretary of Facebook and Chairman of this Annual Meeting, which I now call to order. Let me run through today's agenda. First, we’ll focus on the formal business set forth in the proxy statement. Then, Mark will spend a few minutes talking about how Facebook has been doing and finally, we’ll conclude with the Q&A session.Before we begin with the f\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('facebook_md_transcript.txt', 'r') as f:\n",
    "    text = f.readlines()[0]\n",
    "text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing features\n",
    "\n",
    "Once a vocabulary model has been loaded, text processing is a matter of passing the text into the Language object in the `nlp` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doc'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create object of class Doc\n",
    "# see: https://spacy.io/api/doc\n",
    "doc = nlp(text)\n",
    "type(doc).__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7047"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Good |Lemma: good |P-O-S: ADJ |Dep. Parse: amod |Shape: Xxxx |Stop Word: False |Alphanumeric: True \n",
      "---\n",
      "Token: morning |Lemma: morning |P-O-S: NOUN |Dep. Parse: ROOT |Shape: xxxx |Stop Word: False |Alphanumeric: True \n",
      "---\n",
      "Token: . |Lemma: . |P-O-S: PUNCT |Dep. Parse: punct |Shape: . |Stop Word: False |Alphanumeric: False \n",
      "---\n",
      "Token: Welcome |Lemma: welcome |P-O-S: INTJ |Dep. Parse: ROOT |Shape: Xxxxx |Stop Word: False |Alphanumeric: True \n",
      "---\n",
      "Token: , |Lemma: , |P-O-S: PUNCT |Dep. Parse: punct |Shape: , |Stop Word: False |Alphanumeric: False \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for token in doc[0:5]:\n",
    "    print('Token:', token,\n",
    "          '|Lemma:', token.lemma_,\n",
    "          '|P-O-S:', token.pos_,\n",
    "          '|Dep. Parse:', token.dep_,\n",
    "          '|Shape:', token.shape_,\n",
    "          '|Stop Word:', token.is_stop,\n",
    "          '|Alphanumeric:', token.is_alpha,\n",
    "          '\\n---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes on features\n",
    "* Tokenization and lemmatization: splits by whitespace, but also understands contractions and punctuations\n",
    "* Part-of-speech tagging: use language model to detect P-O-S\n",
    "* Dependency parsing: also uses language model. Useful for resolving ambiguity in text (e.g. \"scientist study whales from space\")\n",
    "* Shape: characterizes shape of token (use case?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "To get named entities, invoke `ents` attribute on `Doc` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: morning |Start index 5 |End index: 12 |Label: TIME\n",
      "Entity: Facebook |Start index 49 |End index: 57 |Label: ORG\n",
      "Entity: 2018 |Start index 60 |End index: 64 |Label: DATE\n",
      "Entity: Annual Meeting of Stockholders |Start index 65 |End index: 95 |Label: WORK_OF_ART\n",
      "Entity: Dave Kling |Start index 101 |End index: 111 |Label: PERSON\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents[0:5]:\n",
    "    print(\"Entity:\", ent.text, \n",
    "          \"|Start index\", ent.start_char, \n",
    "          \"|End index:\", ent.end_char, \n",
    "          \"|Label:\", ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Features\n",
    "\n",
    "The `Doc` object offers other features in addition to the ones demonstrated above. For a full list of features, see `dir(doc)`.\n",
    "\n",
    "Some examples include sentence boundary detection, noun chunks, word vectors, word similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning.\n",
      "Welcome, thanks for joining us for Facebook’s 2018 Annual Meeting of Stockholders.\n",
      "I’m Dave Kling, Deputy General Counsel and Corporate Secretary of Facebook and Chairman of this Annual Meeting, which I now call to order.\n",
      "Let me run through today's agenda.\n",
      "First, we’ll focus on the formal business set forth in the proxy statement.\n"
     ]
    }
   ],
   "source": [
    "# Sentence boundary detection\n",
    "for sent in list(doc.sents)[0:5]:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning\n",
      "us\n",
      "Facebook’s 2018 Annual Meeting\n",
      "Stockholders\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "# Noun chunks\n",
    "for nc in list(doc.noun_chunks)[0:5]:\n",
    "    print(nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384,)\n"
     ]
    }
   ],
   "source": [
    "# doc.vector returns the average vector in the text\n",
    "print(doc.vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.01393139 -1.40702009 -2.64994955  2.09281063 -1.97974396  1.62159455\n",
      "  1.4604938   1.26499486 -1.71690214 -0.38691491 -0.16623339 -0.57863778\n",
      "  0.16432497 -3.3152163  -1.84567595 -0.0419752  -1.46858501 -2.71334958\n",
      "  0.5873363   2.03711629  1.20276451 -0.93468851  0.33614558  1.98867559\n",
      " -1.73306501 -1.80320525 -3.26318407  0.65803003  1.05705953  2.32139993\n",
      "  2.80969214 -1.53222191  0.9180609   0.50728929 -0.67875248  4.46587181\n",
      "  2.92354059  0.30113083 -0.34886426 -1.92665195 -3.7593081   1.07089591\n",
      "  0.16429344 -0.53713918 -0.17663306 -0.52894181 -2.07969904  5.14629459\n",
      " -2.19922924 -3.10876894  0.69134474 -1.29334843 -1.55373967  2.84668589\n",
      " -2.28205085  0.94399607  4.30578947 -0.58578634 -2.3346312  -2.00597453\n",
      "  0.47395083  4.9656105  -0.78003526  0.40823543  2.06077528 -1.84092426\n",
      " -0.62683618  1.5516696  -2.17767739  1.7413547  -1.09436584 -3.12682962\n",
      " -0.03999323 -1.67738426  0.05101991 -0.90884304  2.32613373 -2.54049635\n",
      "  0.95585465 -1.76753855 -0.12845905 -0.21103936 -2.17684293  0.90705669\n",
      " -1.81195068 -1.00439441 -2.67613125  0.53290129 -1.00119531  1.76997054\n",
      " -3.65365672  4.26562214  2.89829731 -2.92895436 -0.42060012  1.51568675\n",
      " -1.72199202  1.87846994  0.3170355   0.204882   -0.96116912 -0.63134921\n",
      " -0.62708974 -0.67105126  2.28814769  0.36261535  1.68345582 -3.37529135\n",
      "  1.79390478  5.91972923 -2.25102735 -0.38480693 -0.7928921   0.31965324\n",
      "  6.96779776  5.02549314 -0.6097703   1.20930123  0.61084759  3.11483216\n",
      "  3.78151083 -0.85041487  3.36690545 -1.15080607 -1.56956887 -2.23750734\n",
      "  1.07527983 -1.45654476 -0.12886384 -0.40896478 -0.29594988 -0.29875267\n",
      " -0.7329573   0.09735585 -0.2569961   0.53502536  0.61913902  0.34328413\n",
      "  0.09850441  0.69298339  1.21383047  0.57170558 -0.61040652 -0.21471392\n",
      "  0.11549481  0.79373974  0.6972388   1.00216103  1.14823472  0.30192918\n",
      "  0.39337575  0.34596592 -0.17566091 -0.46467823 -0.15587392 -0.13884963\n",
      " -0.4142378  -0.47395983  0.3615059   0.50875807 -0.10381863  0.12955575\n",
      " -0.6974858  -0.38711274  0.07860769  0.27208477  0.26912284  0.57113934\n",
      " -0.32099321  0.83191669 -0.02591018  0.01308095  0.3836028   0.38495541\n",
      " -0.36239457  0.1051798  -0.48670942 -0.53156817 -0.03757983 -0.0543268\n",
      " -0.17783478  0.92725492 -0.72980416 -0.30967474  0.02934057 -0.26214236\n",
      "  0.52088082 -0.01216595 -0.41909564 -0.12347949 -0.59600294  0.77775961\n",
      " -0.73353362  0.10160545 -0.21408665 -0.28420603  0.35474473  0.63934809\n",
      " -0.70044792 -0.43117529  0.35899377  0.01613896 -0.04877275  0.76365644\n",
      "  0.29091132 -0.02330117  0.65166366  0.19102539 -0.30764377 -0.17110106\n",
      "  0.06074202  0.01718427 -0.7507022  -0.02601328 -0.80320024 -0.15496497\n",
      " -0.80946481  0.43078929 -0.25598371 -0.42993635 -0.35393742 -0.96515405\n",
      " -0.32988733 -0.54025143  0.5930171  -0.58642316 -0.05325729 -0.44516352\n",
      " -0.5065496   0.60321277 -0.83352172  0.94954562  1.15270865  0.3311002\n",
      " -0.0356673  -0.1558149   0.06313161 -0.11228198 -0.04591939  0.90893209\n",
      "  0.40453964  0.3281002   0.10288811 -0.3125065  -0.39043909  0.08376085\n",
      " -0.22920839 -0.61004221  0.14438222 -0.28363872 -0.31371987 -0.15163563\n",
      "  0.26117584 -0.06534772 -0.10197866 -0.14290361 -0.03169026  1.09773326\n",
      " -0.74288613 -0.39055058  0.55323517 -0.2302673  -0.04724181 -0.18002798\n",
      "  0.96218514 -0.33554161  1.26323664 -0.57879162 -1.03779912  0.22488184\n",
      " -0.19171388 -0.75820678  0.40139365  0.0124113  -0.38860625 -0.1427162\n",
      " -0.64441693 -0.21590093 -0.02177668 -0.27579829 -0.44136429 -0.34000647\n",
      " -0.33255988  0.91509753 -0.6425975   0.15430969  0.11590865  0.09883198\n",
      " -0.91766202 -0.40041599 -0.60025895 -0.08252107  1.25994277 -0.05685197\n",
      " -0.54340148  0.39266694 -0.23813099 -0.08362165 -0.17846912 -0.06042682\n",
      " -0.03264946 -0.20367241  0.90450013 -0.12718034  0.06418909  0.25624615\n",
      "  0.92639744 -0.41550577  0.23555598 -0.17572469 -1.69016063  0.26841232\n",
      "  0.21131288  0.06924987 -0.38545138 -0.1733959   0.54609537  0.54714078\n",
      "  0.64900476 -0.09596269  0.20084187  1.62280715 -0.30472386  0.09011865\n",
      " -0.74865115  0.16529477  1.28234732  0.74735773 -0.57957453  0.06270114\n",
      " -0.06362639  0.32275575 -0.14725758  0.57840437  0.53529102 -0.14369343\n",
      " -0.6109789  -0.31909677  0.16163066  0.0565107   0.35187584 -0.08524603\n",
      "  0.04754344  0.4238103   0.52563846 -0.06217778  0.168589    0.41112432\n",
      "  0.2288892  -0.65391332 -0.50116628 -0.56598556 -0.7955792   0.04472545\n",
      "  0.26154506  0.49486315 -0.33211511 -0.38104898  0.72748321  0.36238521\n",
      "  0.58656675 -0.32481638 -0.06948419 -0.63178331 -0.752361   -0.02150482\n",
      " -0.18672723 -0.05089578  1.01393187 -0.03748867 -0.44796622  0.55182242\n",
      " -0.09411645 -0.29110321 -0.15346608 -0.71470195 -0.31653178 -0.07131719\n",
      "  1.12813044 -0.49486348  0.30231336  0.42120564  0.052669   -0.9382059 ]\n"
     ]
    }
   ],
   "source": [
    "# Get vector of each token\n",
    "for token in doc[0:1]:\n",
    "    print(token.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Good 1.0\n",
      "Good morning 0.14695\n",
      "Good Welcome 0.309286\n",
      "morning Good 0.14695\n",
      "morning morning 1.0\n",
      "morning Welcome 0.377752\n",
      "Welcome Good 0.309286\n",
      "Welcome morning 0.377752\n",
      "Welcome Welcome 1.0\n"
     ]
    }
   ],
   "source": [
    "# Can use word vectors to calculate L2 norm and \n",
    "# to calculate cosine similarity between words\n",
    "for t1 in doc[0:5]:\n",
    "    for t2 in doc[0:5]:\n",
    "        if (len(t1) > 1 and len(t2) > 1):\n",
    "            print(t1, t2, t1.similarity(t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips and Bugs I Encountered\n",
    "1. `is_stop` depends on capitalization\n",
    "    * Example --> The: False, the: true\n",
    "    * Work around: lemmatize words (using `lemma_` method) before using `is_stop`\n",
    "    * Link to issue: https://github.com/explosion/spaCy/issues/1889\n",
    "2. Multi-threading doesn't work (i.e. n_thread > 0 does not make a difference) when using `nlp.pipe`\n",
    "    * Link to issue: https://github.com/explosion/spaCy/issues/2075\n",
    "    * Note on multi-threading in spaCy: https://explosion.ai/blog/multithreading-with-cython\n",
    "3. `similarity` method raises TypeError when single character strings is encountered\n",
    "    * Example in previous cell, above\n",
    "    * Link to issue: https://github.com/explosion/spaCy/issues/2219\n",
    "4. Don't need p-o-s tagging, dependency parsing, or named entity recognition? You can turn them off.\n",
    "    * Disabling and modifying pipeline components: https://spacy.io/usage/processing-pipelines#disabling\n",
    "    * Link to issue: https://github.com/explosion/spaCy/issues/1837"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In summary, the only code you need (after installation) to get started with spaCy are as follows:\n",
    "\n",
    "```python\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"Text to process goes here\")\n",
    "```\n",
    "\n",
    "`nlp(\"Text to process goes here\")` creates the `Doc` object, which contains the tokens of the text. You then access the attributes of your text using the various method calls on each individual `tokens`. Additional features are also available within the created `Doc` object. These can be explored by running `dir(doc)`.\n",
    "\n",
    "Also, the model need not be `en_core_web_sm`. You can choose from [among this list.](https://spacy.io/usage/models#available)\n",
    "\n",
    "See the documentation for even more [detailed and in-depth examples.](https://spacy.io/usage/examples)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
