{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "### How to install\n",
    "Spacy can be installed using either `pip` or `conda`:\n",
    "* `pip install -U spacy`\n",
    "* `conda install -c conda-forge spacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading a Vocabulary Model\n",
    "\n",
    "After importing the library, load one of the many available models. Models may be installed via spaCy's download command\n",
    "\n",
    "```python\n",
    "python -m spacy download <MODEL NAME> \n",
    "\n",
    "```\n",
    "List of available models and available features can be found in the [Available models section of the documentation](https://spacy.io/usage/models#available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary model\n",
    "# 'en_core_web_sm' model: \n",
    "# https://spacy.io/models/en#en_core_web_md\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "type(nlp).__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the components we need to process text. The next step is to pass in the text data into `nlp` and invoke its various methods appropriate for the analysis we want to undertake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Features\n",
    "\n",
    "## Basic things that spaCy can do\n",
    "* Tokenization (word and sentence)\n",
    "* Lemmatization\n",
    "* Part-of-speech tagger\n",
    "* Depdenency parsing\n",
    "* Named entity recognition\n",
    "\n",
    "For full list, see [this page](https://spacy.io/usage/spacy-101#features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('facebook_md_transcript.txt', 'r') as f:\n",
    "    text = f.readlines()[0]\n",
    "text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing features\n",
    "\n",
    "Once a vocabulary model has been, text processing is a matter of passing the text into the Language object bounded to the `nlp` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object of class Doc\n",
    "# see: https://spacy.io/api/doc\n",
    "doc = nlp(text)\n",
    "type(doc).__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print('Token:', token,\n",
    "          '|Lemma:', token.lemma_,\n",
    "          '|P-O-S:', token.pos_,\n",
    "          '|Dep. Parse:', token.dep_,\n",
    "          '|Shape:', token.shape_,\n",
    "          '|Stop Word:', token.is_stop,\n",
    "          '\\n----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes on features\n",
    "* Tokenization and lemmatization: splits by whitespace, but also understands contractions and punctuations\n",
    "* Part-of-speech tagging: use language model to detect POS\n",
    "* Dependency parsing: also uses language model. Useful for resolving ambiguity in text (e.g. \"scientist study whales from space\")\n",
    "* Shape: characterizes shape of token (use case?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "To get named entities, invoke `ents` attribute on `Doc` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Features\n",
    "\n",
    "The `Doc` object offers other features in addition to the ones demonstrated above. For a full list of features, see `dir(doc)`.\n",
    "\n",
    "Some examples include sentence boundary detection, noun chunks, word vectors, word similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence boundary detection\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun chunks\n",
    "for nc in doc.noun_chunks:\n",
    "    print(nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc.vector returns the average vector in the text\n",
    "print(doc.vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vector of each token\n",
    "for token in doc[0:2]:\n",
    "    print(token.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can use word vectors to calculate L2 norm and \n",
    "# to calculate cosine similarity between words\n",
    "for t1 in doc[0:20]:\n",
    "    for t2 in doc[0:20]:\n",
    "        if (len(t1) > 1 and len(t2) > 1):\n",
    "            print(t1, t2, t1.similarity(t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bugs I Encountered\n",
    "1. `is_stop` depends on capitalization\n",
    "    * Example --> The: False, the: true\n",
    "    * Work around: lemmatize words first (using `lemma_` method) before using `is_stop`\n",
    "    * Link to issue: https://github.com/explosion/spaCy/issues/1889\n",
    "2. multi-threading doesn't work (i.e. n_thread > 0 does not make a difference) when using `nlp.pipe`\n",
    "    * Link to issue: https://github.com/explosion/spaCy/issues/2075\n",
    "    * Note on multi-threading in spaCy: https://explosion.ai/blog/multithreading-with-cython\n",
    "3. `similarity` method raises TypeError when single character strings is encountered\n",
    "    * Example in previous cell, above\n",
    "    * Link to issue: https://github.com/explosion/spaCy/issues/2219"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In summary, the only code you need (after installation) to get started with spaCy are as follows:\n",
    "\n",
    "```python\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"Text to process goes here\")\n",
    "```\n",
    "\n",
    "`nlp(\"Text to process goes here\")` creates the `Doc` object, which contains the tokens of the text. You then access the attributes of your text using the various method calls on each individual `tokens`. Additional features are also available within the created `Doc` object. These can be explored by running `dir(doc)`.\n",
    "\n",
    "See the documentation for even more [detailed and in-depth examples.](https://spacy.io/usage/examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "1. [spaCy 101](https://spacy.io/usage/spacy-101)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
